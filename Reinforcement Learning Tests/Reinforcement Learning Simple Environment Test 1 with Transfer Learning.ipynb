{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd1eec2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b3885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b6847f",
   "metadata": {},
   "source": [
    "# Environment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8176d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported data taken from the robot's readings of the entire environment.\n",
    "\n",
    "data = [[94, 103, 128, 469, 443, 420, 420, 437, 641, 471, 447, 446, 464, 540, 540, 138, 137, 156, 191, 137, 103, 99], [101, 107, 130, 483, 483, 433, 429, 448, 649, 464, 448, 439, 456, 498, 813, 130, 126, 138, 166, 198, 109, 107], [103, 109, 143, 490, 446, 429, 429, 446, 645, 460, 446, 443, 455, 500, 964, 130, 130, 143, 198, 204, 109, 103], [103, 112, 145, 504, 446, 428, 428, 452, 649, 456, 441, 441, 455, 511, 158, 121, 121, 143, 205, 138, 114, 112], [103, 112, 139, 496, 443, 433, 433, 454, 649, 460, 439, 444, 460, 496, 157, 126, 126, 149, 191, 191, 109, 103], [121, 121, 157, 203, 139, 120, 120, 143, 500, 452, 427, 420, 446, 661, 464, 439, 439, 456, 483, 510, 126, 121], [120, 118, 162, 199, 139, 118, 120, 149, 500, 447, 435, 435, 454, 661, 460, 439, 439, 455, 500, 1585, 118, 118], [120, 120, 157, 199, 143, 122, 121, 166, 487, 448, 428, 428, 452, 668, 462, 437, 437, 456, 515, 154, 122, 120], [118, 118, 156, 195, 138, 122, 121, 149, 515, 447, 435, 435, 454, 661, 455, 437, 437, 447, 490, 149, 118, 118], [121, 121, 164, 199, 143, 121, 121, 158, 481, 448, 427, 427, 443, 662, 462, 437, 437, 456, 479, 506, 126, 121], [418, 427, 446, 483, 492, 114, 114, 139, 154, 149, 128, 128, 149, 166, 464, 447, 443, 456, 634, 639, 427, 420], [425, 428, 469, 491, 128, 113, 113, 154, 191, 145, 122, 122, 149, 933, 464, 443, 443, 462, 649, 651, 428, 427], [424, 428, 464, 483, 128, 113, 113, 149, 197, 143, 122, 121, 158, 500, 462, 439, 439, 462, 643, 471, 427, 429], [428, 437, 481, 483, 154, 121, 121, 149, 197, 204, 113, 113, 128, 156, 474, 435, 433, 443, 483, 644, 443, 428], [427, 429, 456, 490, 138, 122, 122, 156, 190, 827, 122, 121, 147, 865, 456, 435, 435, 447, 639, 645, 428, 427], [427, 435, 464, 643, 454, 437, 433, 460, 488, 149, 120, 120, 139, 843, 853, 128, 126, 145, 500, 469, 437, 427], [433, 435, 653, 649, 462, 443, 439, 464, 500, 143, 118, 118, 139, 158, 154, 120, 120, 139, 491, 479, 435, 428], [448, 454, 481, 654, 454, 439, 435, 454, 483, 913, 103, 103, 122, 204, 917, 126, 121, 143, 515, 491, 452, 447], [446, 447, 488, 654, 447, 428, 437, 462, 496, 121, 103, 103, 121, 228, 879, 128, 128, 147, 518, 479, 447, 448], [447, 452, 483, 649, 454, 428, 435, 475, 498, 498, 103, 103, 121, 138, 138, 130, 128, 147, 525, 481, 452, 445]]\n",
    "\n",
    "Zone_1 = [data[0], data[1], data[2], data[3], data[4]]\n",
    "Zone_2 = [data[5], data[6], data[7], data[8], data[9]]\n",
    "Zone_3 = [data[10], data[11], data[12], data[13], data[14]]\n",
    "Zone_4 = [data[15], data[16], data[17], data[18], data[19]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988daf0",
   "metadata": {},
   "source": [
    "# Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e776d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainZoneEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take: Guess 1 of 4 zones\n",
    "        self.action_space = Discrete(4)\n",
    "        #  array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([3]))\n",
    "        # Set initial Zone Guess\n",
    "        self.state = 0\n",
    "        # Import the real Zone's sensor data\n",
    "        Zone = random.randint(0,3)\n",
    "        Num = 5*(Zone)\n",
    "        sample = data[Num]\n",
    "        self.sensor_data = sample\n",
    "        # Set amount of guesses per run\n",
    "        self.guess_length = 10\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Difine the sensor values for each Zone\n",
    "        sensor_zone = [Zone_1, Zone_2, Zone_3, Zone_4]\n",
    "        #apply the action\n",
    "        self.state = action\n",
    "        # Reduce guess length by 1\n",
    "        self.guess_length -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        # if the sensor data is in the chosen zone class, reward = 1\n",
    "        if self.sensor_data in sensor_zone[self.state]: \n",
    "            reward = 1\n",
    "            correct = True\n",
    "        else: \n",
    "            reward = -1\n",
    "            correct = False\n",
    "        \n",
    "        # Check if guessing is done\n",
    "        if self.guess_length <= 0 or correct == True: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset Initial Zone Guess\n",
    "        self.state = 0\n",
    "        # Import a new Zone's sensor data\n",
    "        Zone = random.randint(0,3)\n",
    "        Num = 5*(Zone)\n",
    "        sample = data[Num]\n",
    "        self.sensor_data = sample\n",
    "        # Reset Guess amount\n",
    "        self.guess_length = 10\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfadae5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-2 zone guess 3\n",
      "Episode:2 Score:-1 zone guess 1\n",
      "Episode:3 Score:-3 zone guess 3\n",
      "Episode:4 Score:-8 zone guess 4\n",
      "Episode:5 Score:1 zone guess 2\n",
      "Episode:6 Score:0 zone guess 1\n",
      "Episode:7 Score:1 zone guess 4\n",
      "Episode:8 Score:1 zone guess 2\n",
      "Episode:9 Score:1 zone guess 2\n",
      "Episode:10 Score:1 zone guess 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\gym\\spaces\\box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "env = TrainZoneEnv()\n",
    "\n",
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{} Zone guess:{}'.format(episode, score, n_state+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d636de4",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e41b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e317b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17000/3682394117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03e69e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 24)                48        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                600       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 748\n",
      "Trainable params: 748\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29efdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8f7c9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402b71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "\r",
      "    1/10000 [..............................] - ETA: 11:42 - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   40/10000 [..............................] - ETA: 2:30 - reward: -0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
      "C:\\Users\\Dylan\\anaconda3\\lib\\site-packages\\rl\\memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
      "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 60s 6ms/step - reward: -0.4578\n",
      "2818 episodes - episode_reward: -1.624 [-10.000, 1.000] - loss: 0.738 - mae: 1.115 - mean_q: -0.865\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 65s 6ms/step - reward: -0.4540\n",
      "2841 episodes - episode_reward: -1.599 [-10.000, 1.000] - loss: 0.827 - mae: 1.205 - mean_q: -0.979\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 65s 7ms/step - reward: -0.4424\n",
      "2881 episodes - episode_reward: -1.534 [-10.000, 1.000] - loss: 0.852 - mae: 1.236 - mean_q: -1.020\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 66s 7ms/step - reward: -0.4504\n",
      "2846 episodes - episode_reward: -1.584 [-10.000, 1.000] - loss: 0.848 - mae: 1.224 - mean_q: -1.006\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 67s 7ms/step - reward: -0.4432\n",
      "done, took 323.779 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5ea869460>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085973ef",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96a98374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestZoneEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take: Guess 1 of 4 zones\n",
    "        self.action_space = Discrete(4)\n",
    "        #  array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([3]))\n",
    "        # Set initial Zone Guess\n",
    "        self.state = 0\n",
    "        # Import the real Zone's sensor data\n",
    "        sample = data[Num]\n",
    "        self.sensor_data = sample\n",
    "        # Set amount of guesses per run\n",
    "        self.guess_length = 10\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Difine the sensor values for each Zone\n",
    "        sensor_zone = [Zone_1, Zone_2, Zone_3, Zone_4]\n",
    "        #apply the action\n",
    "        self.state = action\n",
    "        # Reduce guess length by 1\n",
    "        self.guess_length -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        # if the sensor data is in the chosen zone class, reward = 1\n",
    "        if self.sensor_data in sensor_zone[self.state]: \n",
    "            reward = 1\n",
    "            correct = True\n",
    "        else: \n",
    "            reward = -1\n",
    "            correct = False\n",
    "        \n",
    "        # Check if guessing is done\n",
    "        if self.guess_length <= 0 or correct == True: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset Initial Zone Guess\n",
    "        self.state = 0\n",
    "        # Import a new Zone's sensor data\n",
    "        sample = data[Num]\n",
    "        self.sensor_data = sample\n",
    "        # Reset Guess amount\n",
    "        self.guess_length = 10\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e554fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the zone the Agent is in.\n",
    "Zone = 4\n",
    "Num = 5*(Zone -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d6002e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 1.000, steps: 1\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Test to determine if the Agent knows where it is.\n",
    "env = TestZoneEnv()\n",
    "\n",
    "scores = dqn.test(env, nb_episodes=1, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bc78f",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8fda6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferZoneEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take: Guess 1 of 4 zones\n",
    "        self.action_space = Discrete(4)\n",
    "        #  array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([3]))\n",
    "        # Set initial Zone Guess\n",
    "        self.state = 0\n",
    "        # Import the real Zone's sensor data\n",
    "        self.sensor_data = sample\n",
    "        # Set amount of guesses per run\n",
    "        self.guess_length = 10\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Difine the sensor values for each Zone\n",
    "        sensor_zone = [Zone_1, Zone_2, Zone_3, Zone_4]\n",
    "        #apply the action\n",
    "        self.state = action\n",
    "        # Reduce guess length by 1\n",
    "        self.guess_length -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        # if the sensor data is in the chosen zone class, reward = 1\n",
    "        if self.sensor_data in sensor_zone[self.state]: \n",
    "            reward = 1\n",
    "            correct = True\n",
    "        else: \n",
    "            reward = -1\n",
    "            correct = False\n",
    "        \n",
    "        # Check if guessing is done\n",
    "        if self.guess_length <= 0 or correct == True: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset Initial Zone Guess\n",
    "        self.state = 0\n",
    "        # Import a new Zone's sensor data\n",
    "        self.sensor_data = sample\n",
    "        # Reset Guess amount\n",
    "        self.guess_length = 10\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d980f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sensor data from the robot\n",
    "\n",
    "sample = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "622fd350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: -10.000, steps: 10\n",
      "-10.0\n"
     ]
    }
   ],
   "source": [
    "TrEnv = TransferZoneEnv()\n",
    "\n",
    "scores = dqn.test(TrEnv, nb_episodes=1, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
